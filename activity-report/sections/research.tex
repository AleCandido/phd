Parton Distribution Functions (\pdf{}s) are a description of the collinear
structure of the proton, and possibly other hadrons appearing as input states
of the high energy processes.
Since \pdf{}s are generated by the non-perturbative \qcd{} dynamics, it is not
possible, at the state of the art, to compute them from first principles. Thus
we are fitting them.

There are three main ingredients that are essential for \pdf{} fit: data,
theory predictions, and the fitting machinery.

\subsection*{Theory Framework}

Theory predictions have always been performed in \nnpdf{} - \pdf{} fits more in
general - by a rather composite ecosystem of different programs. Because of
this, it has been developed an interest for improving part of it, and
integrating the separate pieces into a single pipeline.

\dglap{} evolution determines the \pdf{}s values at any scale from a given
border condition at an initial scale.
It is thus crucial for fitting, since it is needed to compare the candidates
and final determination from the fit at the chosen scale, to the experimental
data that are at many different scales.

Several different options can affect the actual \dglap{} solution, and for this
reason you want a reliable and extendable tool, able to deliver a solution,
reusable over and over with a limited cost in computing time.

For this reason we developed \textbf{\eko{}}
\citepap{Candido:2022tld}\citecode{candido_alessandro_2022_6340153}, in order
to improve some of the features of the former program used (i.e.\ \apfel{}),
and to include a few requirements at design level: compute the solution as a
linear operator (as opposed to evolve an initial condition over and over),
solve the equation in Mellin space, and have a maintainable and extendable
code, to help future contributors.

In the same spirit, and at the same time, we did something similar in the
development of \textbf{\yadism{}} \citecode{candido_alessandro_2022_6285149}, a
provider for \dis{} predictions (structure functions and reduced cross
sections).

Formerly, \dis{} had a very special role, being the process that was mainly
constraint the \pdf{}s themselves, especially driven by \hera{} data.
This was reflected in programs like \apfel{} and \qcdnum{} dispatching both
evolution and \dis{} predictions.
This is not any longer true with the advent of \lhc{} data, and many different
processes are now included in the fit. Each one playing a different and
important role as a part of the global dataset.

With all this in mind, a collaborator (C.\ Schwan), started writing
\textbf{\pineappl{}} \citecode{christopher_schwan_2022_7023438}, a common
interface for many \pdf{}s agnostic calculations.
I personally contributed the Python bindings and \eko{} interface, that is part
of the main library (being evolution in common to all processes), while I also
implemented a \yadism{} interface to \pineappl{}. But being \dis{} only one of
the processes this is on the side of \yadism{} itself.

In order to smoothen the process of combining \eko{} outputs with \pineappl{}
grids, we wrote \textbf{\pineko{}}\citecode{andrea_barontini_2022_7093030}, a
further code taking care of the whole process, including the computation of the
missing \eko{} operators.

Finally, \yadism{} is not the only code interfaced to \pineappl{}, and working
as the provider.
Because of the different programs it is more and more difficult to repeat the
calculation of all the predictions required for the fit. Thus, we store all the
\textbf{runcards} in a dedicated repository
\citecode{christopher_schwan_2022_7093057}.
We also provide one more program, \textbf{\pinefarm{}}, able to install all the
dependencies for said calculations, and reproduce the results given just the
runcard.

A description of the main components, together with a first design of the
overall layout of the theory framework, has also been contributed to the
Snowmass process \citepap{Amoroso:2022eow}.

\subsection*{Applications}

One of the first applications of \eko{}, has been the determination of a heavy
non-perturbative charm component of the proton, the so called \textbf{intrinsic
charm} \citepap{Ball:2022qks}.
This is result is based on the last release of \nnpdf{} family, i.e.\
\nnpdf{4.0}, and the \eko{} bacward evolution, together with its treatment of
intrinsic flavors, and the availability of \nnnlo matching conditions.

Another relevant study (yet unpublished), consisted in a parametrization for
\textbf{neutrinos structure functions}, based on the low energy neutrino \dis{}
data, and the \pqcd{} predictions of \yadism{}, still using \nnpdf{4.0} as
\pdf{} set.

\subsection*{Further studies}

It has of main interest for fitting to implement as many theoretical
constraints as possible, to sharpen our knowledge of the unknown object, and
reduce the amount of degrees of freedom only constrained by experimental data.
This is why we attempted to prove \textbf{\pdf{}s positivity}
\citepap{Candido:2020yat,Candido:2021zcp} through \pqcd{} arguments.
It was already assumed as part of the main \pdf{} fits (notably not
\nnpdf{3.1}), and it has been one of the novelties introduced in \nnpdf{4.0}.
Though later disputed, the main controversy is about a regime in which the data
are also potentially affected by non-perturbative effects, and thus already
mostly excluded from the fit itself.

A further study, still ongoing, is about the possible \textbf{sampling} from
the determined \pdf{}s distribution, without the support of a neural network,
nor a fixed parametrization over a polynomial (or similar) basis.
The idea is to directly use the values of the \pdf{}s at given points of
interest, and to obtain their distribution by \textbf{Bayesian inference}, by
means of an approximate solution (based on a Gaussian approximation) or
posterior sampling methods.

Finally, I took part in a detailed investigation of large $x$ \pdf{}s
\citepap{Ball:2022qtp}.
We focused on the \textbf{forward-backward asymmetry} $A_{fb}$ in Drell-Yan
distribution in Collins-Soper angle, and we analyzed how the features of this
distribution change for a high cut in the invariant mass of the dilepton pair.
We traced back this behavior to the underlying \pdf{}s features, pointing out
how it is sensitive to the large $x$ behavior (for a high enough cut), and thus
the related extrapolation procedure (even though in a non-trivial flavor
combination).
