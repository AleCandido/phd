% !TeX root = ../../../main.tex

The goal of \mhou studies is to give an estimate of the impact of the missing
part of the perturbative series, in order to assess the size theory uncertainty
propagated on physical observables.
%
There are two categories of possible approaches: use all-order information
coming from theoretical knowledge of the perturbative series (or properties
that applies to the all-order result) and extrapolating from the behavior of
the known orders.

The first category makes use of a similar information of that exploited in
\textit{resummation}, with a different goal: resumming the perturbative series
produces a new expansion with a better convergence, while in \mhou studies the
goal is to estimate the missing part of the initial truncated series.
%
The prominent example of this category is the widespread adoption of
\textbf{scale variations} as theory uncertainties on perturbative calculations.
The physical motivation relies in Callan-Symanzik equations, the same used to
obtain \dglap (cf.\ \cref{sec:qcd/dglap}).
%
These equations encode a property of physical observables: they can not depend
on unphysical scales.
But this property holds only for the all-order physical observables, and it is
spoilt by the perturbative truncation.
Therefore, measuring the dependence of the final result on the variation of
unphysical scales, it is possible to extract the magnitude of this violation. 
%
It is not possible to reconstruct from the exact value of an observable from
this information: many missing terms do not belong to the same \textit{class}
of known ones, since it is possible to group terms in such a way that
Callan-Symanzik equations are respected for each group, but only the sum of all
groups value is the value of the full series.
%
Nevertheless, as said before the goal of \mhou investigations is not to upgrade
a truncated result to a full one, so capturing the order of magnitude is
sufficient.
%
There are cases in which the scale variations approach is known to fail, giving
an unreliable estimate also of the order of magnitude.
However, most of these cases can be predicted by simple enough properties of
the perturbative series.
E.g.\ at low enough orders some partonic channels might not be present yet,
like \dis at \lo has no gluon channel (and at \nlo no quark singlet
contribution).
%
There is not a single scale to be varied, but two: the renormalization and the
factorization scales, they have been briefly introduced in
\cref{sec:qcd/dglap}, and they also appeared in \cref{ch:dis,ch:eko}.
They are linked to the two perturbative truncations described above, so the two
of them have to be varied to obtain a complete estimate.
The way the two variations are coordinated is called a scale variations
\textit{prescription} and it is illustrated in more details in
\cref{sec:mhou/prescriptions}. 

The main criticism to \textit{scale variations} is not to capture only a subset
of missing terms, that is mostly common to all approaches since the available
information is coming from the finite amount of computed terms.
Instead, it is the \textit{arbitrariness} connected to the prescriptions
themselves.
%
Even for single scale variation there is already a completely free parameter:
the amount of the variation.
%
The conventional solution, based on the logarithmic nature of the scale
dependence, is to double and halve the value of the scale, usually set to a
process scale, to minimize \enquote{spurious} contributions (but the chosen
scale is also somewhat arbitrary, for the same reason).
%
This does not solve the arbitrariness that remains in the connection between
the estimate and the real value, but it gives a way to compare the impact on
different calculations, since the two estimates will share the same arbitrary
value.

The second category began with \cite{Cacciari:2011ze}, that formulated a
Bayesian model to extrapolate the prediction value for unknown orders, updating
it with the known ones.
%
There are two main goals for this kind of approaches: getting rid of the
scale variations arbitrariness, and extract more information than a single
shift estimate.
Indeed, the result of Bayesian inference is always a posterior distribution of
some quantities, or something derived from it.
%
The probability density in principle contains more information, and it does not
have to rely on some Gaussian assumption (as will be done for example in
\cref{sec:mhou/pdf}), that while reasonable in most situations, can drastically
fail for some edge cases.
%
But in order to infer these quantities, two inputs are required: a prior
distribution and a conditional likelihood, and unless some theoretical
knowledge about the perturbative series is embedded in their definition, they
are arbitrary as well.

In that work in particular, the authors assume that the one and only link
between different perturbative orders is a common upper bound, dubbed
$\bar{c}$.
They assume a certain distribution for the orders, dependent uniquely on this
parameter, and motivated only by its simplicity (admittedly forced because of
the simplification of the result), and assume a flat non-normalizable prior in
$\log(\bar{c})$, and try to estimate the unknown orders passing through the
distribution of the $\bar{c}$ parameter.
%
An entire section is spent motivating the various choices, but the guiding
principles are just simplicity and technical benefits.
So they avoid the choice of quantitative arbitrary parameters, but trading for
the choice of qualitatively arbitrary functions.
%
This approach has been further investigated by other authors, proposing
empirical Bayes methods to set the optimal scale of the parameter $\bar{c}$
\cite{Forte:2013mda}, considering different models, with more constraining
assumptions, or consuming the same information contained in scale variations
approach \cite{Bonvini:2020xeo}.
Another option is always to increase the number of parameters
\cite{Duhr:2021mfd}, to obtain a different compromise between flexibility and
simplicity.

All these approaches are designed to be applied on observables predictions, but
they need some further methodological developments to be propagated on the \pdf
determination.
In the following, I will focus on the \textit{theory covariance matrix}
method, introduced by the \nnpdf collaboration, but this is not the only option
available, since recently other approaches have been proposed, based on
probabilistic reweighting and post-fit selection \cite{Kassabov:2022orn} or the
introduction of nuisance parameters in the fit \cite{McGowan:2022nag}.
